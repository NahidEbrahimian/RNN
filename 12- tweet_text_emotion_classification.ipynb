{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem: RNN Text Classification"
      ],
      "metadata": {
        "id": "gVmZOWRrBF8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OkJKxn3wgkvM"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, LSTM, GRU\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tweet_emotions Dataset\n",
        "\n",
        "- X contains 40000 sentences (strings)\n",
        "- Y contains string labels in 13 classes\n",
        "\n",
        "Dataset link:\n",
        "\n",
        "https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download\n"
      ],
      "metadata": {
        "id": "dJPA0QLqB1XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read csv file and split data\n",
        "data_frame = pd.read_csv(\"/content/drive/MyDrive/dataset/tweet_emotions.csv\")\n",
        "\n",
        "data_train = data_frame[:30000]\n",
        "data_test = data_frame[30000:]\n",
        "\n",
        "# Get all classes\n",
        "all_classes = data_train.sentiment.unique().tolist()\n",
        "print(all_classes)\n",
        "\n",
        "# Get max length of sentences\n",
        "max_len = len(max(np.array(data_frame[\"content\"]), key=len).split(\" \"))\n",
        "max_len"
      ],
      "metadata": {
        "id": "odVepcWahpOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4deecc-14e2-4923-c0a1-b7e46b7d09ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise', 'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to number\n",
        "le = LabelEncoder()\n",
        "\n",
        "Y_train = le.fit_transform(data_train[\"sentiment\"])\n",
        "Y_test = le.fit_transform(data_test[\"sentiment\"])\n",
        "Y_train, len(Y_train), Y_test, len(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sgLTV3hmqOe",
        "outputId": "66479bc7-70ea-4405-cdfb-6eff342dc9fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 2, 10, 10, ...,  5,  4,  5]),\n",
              " 30000,\n",
              " array([ 8, 12,  8, ...,  7,  5,  7]),\n",
              " 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data from dataframe\n",
        "X_train = data_train[\"content\"]\n",
        "X_test = data_test[\"content\"]"
      ],
      "metadata": {
        "id": "NvzCaKvc_lMC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace labels with related emoji\n",
        "def label_to_emoji(label):\n",
        "    emojies = [\"ü•î\", \"üòû\", \"üòÑ\", \"üôÇ\", \"üòß\", \"ü§©\", \"‚ù§Ô∏è\", \"üòÇ\", \"ü§¢\", \"üòÜ\", \"üòì\", \"üòå\", \"üò°\"]\n",
        "    return emojies[label]\n",
        "\n",
        "index = 200\n",
        "print(X_train[index], label_to_emoji(Y_train[index]))"
      ],
      "metadata": {
        "id": "9Z3RPSgFhrDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ec03c7-842f-401a-f782-d449cb0c4f4e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@AlexanderGWhite daaammmnnnnn I do wish I was there. üòì\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emojifier-V1\n",
        "\n",
        "Each word has some feature, and in Emojifier-V1 we want to classify sentences using multilayer perceptron:\n",
        "\n",
        "- We get the average of words in each sentence and then forward it to the multilayer perceptron with 50 input neurons(each word has 50 features, then the average of words in the sentence has 50 features) and an output layer of softmax with 5 neurons.\n",
        "\n",
        "- For feature vectors, we can get from this link: http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/Alireza-Akhavan/rnn-notebooks/blob/master/images/image_1.png?raw=1\" style=\"width:900px;height:300px;\">\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "LflrvFq-FXdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdJT5P6KLRfj",
        "outputId": "69a248f0-e904-45c3-f1ff-1c29620b2190"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one hot\n",
        "num_classes = len(np.unique(Y_train))\n",
        "\n",
        "Y_train_oh = tf.keras.utils.to_categorical(Y_train, num_classes)\n",
        "Y_test_oh = tf.keras.utils.to_categorical(Y_test, num_classes)\n",
        "num_classes"
      ],
      "metadata": {
        "id": "USU58amfjtaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb1057d-e8b0-405b-947c-4267d276911c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 5\n",
        "print(Y_train[index], \"is converted into one hot\", Y_train_oh[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKJfkEXBPq6J",
        "outputId": "43dadb20-ddb8-4c59-aa39-00e077b20ff1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 is converted into one hot [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download feature vectors and extract\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip -d glov.6B"
      ],
      "metadata": {
        "id": "G2YXsxC5hvvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72129197-b7c0-4467-c64e-e7849326da1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-10 20:16:26--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-11-10 20:16:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-11-10 20:16:27--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‚Äòglove.6B.zip‚Äô\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 39s  \n",
            "\n",
            "2022-11-10 20:19:07 (5.16 MB/s) - ‚Äòglove.6B.zip‚Äô saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read feature vectors and save them\n",
        "\"\"\"\n",
        "In the text file, in each line,\n",
        "the word comes first, and then the feature vectors(each word is in one line).\n",
        "\"\"\"\n",
        "def read_glov_vectors(glove_file):\n",
        "  f = open(glove_file, encoding=\"utf8\")\n",
        "  words = set()\n",
        "  words_to_vec = dict()\n",
        "  for line in f:\n",
        "    line = line.strip().split()\n",
        "    word = line[0]\n",
        "    vec = line[1:]\n",
        "    words.add(word)\n",
        "    words_to_vec[word] = np.array(vec, dtype=np.float64)\n",
        "  return words_to_vec"
      ],
      "metadata": {
        "id": "rtQ_L_0Tiqcm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_vec = read_glov_vectors(\"/content/glov.6B/glove.6B.50d.txt\")\n",
        "\n",
        "# Test the output of read_glov_vectors function\n",
        "words_to_vec[\"hello\"]"
      ],
      "metadata": {
        "id": "JG-Yx2ixlshd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a13918a-f5ce-4713-f2b1-c7100f613e7c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.38497 ,  0.80092 ,  0.064106, -0.28355 , -0.026759, -0.34532 ,\n",
              "       -0.64253 , -0.11729 , -0.33257 ,  0.55243 , -0.087813,  0.9035  ,\n",
              "        0.47102 ,  0.56657 ,  0.6985  , -0.35229 , -0.86542 ,  0.90573 ,\n",
              "        0.03576 , -0.071705, -0.12327 ,  0.54923 ,  0.47005 ,  0.35572 ,\n",
              "        1.2611  , -0.67581 , -0.94983 ,  0.68666 ,  0.3871  , -1.3492  ,\n",
              "        0.63512 ,  0.46416 , -0.48814 ,  0.83827 , -0.9246  , -0.33722 ,\n",
              "        0.53741 , -1.0616  , -0.081403, -0.67111 ,  0.30923 , -0.3923  ,\n",
              "       -0.55002 , -0.68827 ,  0.58049 , -0.11626 ,  0.013139, -0.57654 ,\n",
              "        0.048833,  0.67204 ])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Convert sentences to the average of the word vectors\n",
        "def sentence_to_avg(sentence):\n",
        "  words = sentence.lower().split() # Convert uppercase to lowercase\n",
        "  sum_vectors = np.zeros((50, ))\n",
        "  for w in words:\n",
        "    # if w.startswith('@'):\n",
        "    #   continue\n",
        "    # else\n",
        "    try:\n",
        "      sum_vectors += words_to_vec[w]\n",
        "    except:\n",
        "      pass\n",
        "  avg_vectors = sum_vectors / len(words)\n",
        "  return avg_vectors"
      ],
      "metadata": {
        "id": "zPCRo5VumAQH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test sentence_to_avg function\n",
        "sentence_to_avg(\"Pasta is my favorite food\")"
      ],
      "metadata": {
        "id": "2HnS59mns5x-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29abe21b-8cbd-489f-cc39-01c044663f39"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.242832  ,  0.370774  , -0.524396  ,  0.018644  ,  0.568756  ,\n",
              "        0.0219878 , -0.48206322, -0.152204  ,  0.235412  ,  0.1979466 ,\n",
              "       -0.178818  ,  0.3203976 ,  0.3379962 ,  0.1399654 ,  0.56775044,\n",
              "        0.118648  , -0.04531252,  0.335416  ,  0.149832  , -0.522814  ,\n",
              "        0.095746  , -0.0468764 ,  0.5508066 ,  0.39369132,  0.275182  ,\n",
              "       -1.275018  , -0.76076   ,  0.449102  ,  0.7542772 , -0.2332608 ,\n",
              "        2.82554   ,  0.287742  , -0.325976  ,  0.608572  , -0.020543  ,\n",
              "        0.286476  , -0.24984   ,  0.899408  ,  0.38995   , -0.270266  ,\n",
              "        0.3004734 ,  0.315962  , -0.2408782 ,  0.1586226 ,  0.5400462 ,\n",
              "        0.412066  , -0.1657008 , -0.253566  ,  0.3091806 ,  0.371192  ])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the average of all sentences\n",
        "X_train_avg = []\n",
        "for i in range(X_train.shape[0]):\n",
        "  X_train_avg.append(sentence_to_avg(X_train[i].strip()))\n",
        "\n",
        "X_train_avg = np.array(X_train_avg)\n",
        "\n",
        "X_train_avg.shape, Y_train_oh.shape"
      ],
      "metadata": {
        "id": "CyUggXaVnhpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845f6b21-630c-44d4-d7b0-c25120a51381"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30000, 50), (30000, 13))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model(using perceptron)\n",
        "class EmojiNet_V1(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense = Dense(num_classes, input_shape=(50,), activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8tueM3X_HeWK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit the model\n",
        "model = EmojiNet_V1()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_avg, Y_train_oh, batch_size=64, epochs=300, shuffle=True)"
      ],
      "metadata": {
        "id": "bjZk2ZJRHhKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154152b3-a785-404d-a1bb-2eef042f2f8e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "469/469 [==============================] - 4s 2ms/step - loss: 2.1785 - accuracy: 0.2358\n",
            "Epoch 2/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0833 - accuracy: 0.2715\n",
            "Epoch 3/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0597 - accuracy: 0.2809\n",
            "Epoch 4/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0452 - accuracy: 0.2860\n",
            "Epoch 5/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0348 - accuracy: 0.2902\n",
            "Epoch 6/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0273 - accuracy: 0.2914\n",
            "Epoch 7/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0212 - accuracy: 0.2920\n",
            "Epoch 8/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0164 - accuracy: 0.2923\n",
            "Epoch 9/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0125 - accuracy: 0.2941\n",
            "Epoch 10/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0092 - accuracy: 0.2939\n",
            "Epoch 11/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0067 - accuracy: 0.2954\n",
            "Epoch 12/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0045 - accuracy: 0.2963\n",
            "Epoch 13/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0024 - accuracy: 0.2958\n",
            "Epoch 14/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0010 - accuracy: 0.2958\n",
            "Epoch 15/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9994 - accuracy: 0.2968\n",
            "Epoch 16/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9980 - accuracy: 0.2961\n",
            "Epoch 17/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9973 - accuracy: 0.2982\n",
            "Epoch 18/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9962 - accuracy: 0.2963\n",
            "Epoch 19/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9952 - accuracy: 0.2982\n",
            "Epoch 20/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9944 - accuracy: 0.2977\n",
            "Epoch 21/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9939 - accuracy: 0.2968\n",
            "Epoch 22/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9933 - accuracy: 0.2979\n",
            "Epoch 23/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9928 - accuracy: 0.2980\n",
            "Epoch 24/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9921 - accuracy: 0.2980\n",
            "Epoch 25/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9918 - accuracy: 0.2978\n",
            "Epoch 26/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9914 - accuracy: 0.2983\n",
            "Epoch 27/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9910 - accuracy: 0.2985\n",
            "Epoch 28/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9905 - accuracy: 0.2985\n",
            "Epoch 29/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9904 - accuracy: 0.2991\n",
            "Epoch 30/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9904 - accuracy: 0.2980\n",
            "Epoch 31/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9897 - accuracy: 0.3004\n",
            "Epoch 32/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9897 - accuracy: 0.2982\n",
            "Epoch 33/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9895 - accuracy: 0.2987\n",
            "Epoch 34/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9891 - accuracy: 0.2981\n",
            "Epoch 35/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9892 - accuracy: 0.2995\n",
            "Epoch 36/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9890 - accuracy: 0.2977\n",
            "Epoch 37/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9887 - accuracy: 0.2978\n",
            "Epoch 38/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9885 - accuracy: 0.2981\n",
            "Epoch 39/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9887 - accuracy: 0.2974\n",
            "Epoch 40/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9882 - accuracy: 0.2981\n",
            "Epoch 41/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9883 - accuracy: 0.2985\n",
            "Epoch 42/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9881 - accuracy: 0.2988\n",
            "Epoch 43/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9880 - accuracy: 0.2991\n",
            "Epoch 44/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9880 - accuracy: 0.2994\n",
            "Epoch 45/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9882 - accuracy: 0.2982\n",
            "Epoch 46/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9877 - accuracy: 0.2987\n",
            "Epoch 47/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9876 - accuracy: 0.3000\n",
            "Epoch 48/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9877 - accuracy: 0.2980\n",
            "Epoch 49/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9875 - accuracy: 0.2995\n",
            "Epoch 50/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9877 - accuracy: 0.2985\n",
            "Epoch 51/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9875 - accuracy: 0.2986\n",
            "Epoch 52/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9873 - accuracy: 0.2979\n",
            "Epoch 53/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9870 - accuracy: 0.2992\n",
            "Epoch 54/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9873 - accuracy: 0.2991\n",
            "Epoch 55/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9871 - accuracy: 0.2991\n",
            "Epoch 56/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9870 - accuracy: 0.2992\n",
            "Epoch 57/300\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.9871 - accuracy: 0.2981\n",
            "Epoch 58/300\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.9869 - accuracy: 0.2993\n",
            "Epoch 59/300\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.9870 - accuracy: 0.2987\n",
            "Epoch 60/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9869 - accuracy: 0.2989\n",
            "Epoch 61/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9869 - accuracy: 0.2994\n",
            "Epoch 62/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9868 - accuracy: 0.2998\n",
            "Epoch 63/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9870 - accuracy: 0.2990\n",
            "Epoch 64/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9867 - accuracy: 0.2984\n",
            "Epoch 65/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9868 - accuracy: 0.2996\n",
            "Epoch 66/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9864 - accuracy: 0.2994\n",
            "Epoch 67/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9866 - accuracy: 0.3002\n",
            "Epoch 68/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9866 - accuracy: 0.2990\n",
            "Epoch 69/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9865 - accuracy: 0.3001\n",
            "Epoch 70/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9865 - accuracy: 0.2986\n",
            "Epoch 71/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9864 - accuracy: 0.2978\n",
            "Epoch 72/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9867 - accuracy: 0.3002\n",
            "Epoch 73/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9866 - accuracy: 0.2999\n",
            "Epoch 74/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9861 - accuracy: 0.2989\n",
            "Epoch 75/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9862 - accuracy: 0.2995\n",
            "Epoch 76/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9865 - accuracy: 0.2982\n",
            "Epoch 77/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9863 - accuracy: 0.2984\n",
            "Epoch 78/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9864 - accuracy: 0.2985\n",
            "Epoch 79/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9861 - accuracy: 0.2995\n",
            "Epoch 80/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9863 - accuracy: 0.2990\n",
            "Epoch 81/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9861 - accuracy: 0.2998\n",
            "Epoch 82/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9862 - accuracy: 0.2985\n",
            "Epoch 83/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9857 - accuracy: 0.2992\n",
            "Epoch 84/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9863 - accuracy: 0.2987\n",
            "Epoch 85/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9863 - accuracy: 0.2990\n",
            "Epoch 86/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9863 - accuracy: 0.2993\n",
            "Epoch 87/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9860 - accuracy: 0.2982\n",
            "Epoch 88/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9861 - accuracy: 0.2991\n",
            "Epoch 89/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9862 - accuracy: 0.2992\n",
            "Epoch 90/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9862 - accuracy: 0.2996\n",
            "Epoch 91/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9861 - accuracy: 0.2992\n",
            "Epoch 92/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9860 - accuracy: 0.2985\n",
            "Epoch 93/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9859 - accuracy: 0.2998\n",
            "Epoch 94/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9859 - accuracy: 0.2975\n",
            "Epoch 95/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9859 - accuracy: 0.2999\n",
            "Epoch 96/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9859 - accuracy: 0.2992\n",
            "Epoch 97/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9859 - accuracy: 0.2983\n",
            "Epoch 98/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9860 - accuracy: 0.2989\n",
            "Epoch 99/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9861 - accuracy: 0.2989\n",
            "Epoch 100/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9856 - accuracy: 0.2996\n",
            "Epoch 101/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.2992\n",
            "Epoch 102/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9859 - accuracy: 0.3003\n",
            "Epoch 103/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9858 - accuracy: 0.2998\n",
            "Epoch 104/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9857 - accuracy: 0.2987\n",
            "Epoch 105/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9858 - accuracy: 0.2998\n",
            "Epoch 106/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9856 - accuracy: 0.3001\n",
            "Epoch 107/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9859 - accuracy: 0.2996\n",
            "Epoch 108/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9858 - accuracy: 0.2997\n",
            "Epoch 109/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9858 - accuracy: 0.2995\n",
            "Epoch 110/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9857 - accuracy: 0.2999\n",
            "Epoch 111/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9859 - accuracy: 0.2988\n",
            "Epoch 112/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9856 - accuracy: 0.2994\n",
            "Epoch 113/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9858 - accuracy: 0.2984\n",
            "Epoch 114/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.3002\n",
            "Epoch 115/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9858 - accuracy: 0.2992\n",
            "Epoch 116/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9857 - accuracy: 0.2987\n",
            "Epoch 117/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2998\n",
            "Epoch 118/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9856 - accuracy: 0.2984\n",
            "Epoch 119/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9857 - accuracy: 0.3000\n",
            "Epoch 120/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2996\n",
            "Epoch 121/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9857 - accuracy: 0.2989\n",
            "Epoch 122/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2988\n",
            "Epoch 123/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2992\n",
            "Epoch 124/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.3000\n",
            "Epoch 125/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2994\n",
            "Epoch 126/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9857 - accuracy: 0.3002\n",
            "Epoch 127/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.2992\n",
            "Epoch 128/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.2992\n",
            "Epoch 129/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9856 - accuracy: 0.2986\n",
            "Epoch 130/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2990\n",
            "Epoch 131/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.2992\n",
            "Epoch 132/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2997\n",
            "Epoch 133/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.3000\n",
            "Epoch 134/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2995\n",
            "Epoch 135/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.2988\n",
            "Epoch 136/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9856 - accuracy: 0.2987\n",
            "Epoch 137/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.3001\n",
            "Epoch 138/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.2990\n",
            "Epoch 139/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2984\n",
            "Epoch 140/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2993\n",
            "Epoch 141/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.3003\n",
            "Epoch 142/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.3000\n",
            "Epoch 143/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9855 - accuracy: 0.2989\n",
            "Epoch 144/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2988\n",
            "Epoch 145/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.2990\n",
            "Epoch 146/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.3001\n",
            "Epoch 147/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.2996\n",
            "Epoch 148/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.2991\n",
            "Epoch 149/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2995\n",
            "Epoch 150/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.2990\n",
            "Epoch 151/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2998\n",
            "Epoch 152/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.2992\n",
            "Epoch 153/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.2995\n",
            "Epoch 154/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2992\n",
            "Epoch 155/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.3000\n",
            "Epoch 156/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.2991\n",
            "Epoch 157/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.3001\n",
            "Epoch 158/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2995\n",
            "Epoch 159/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.2992\n",
            "Epoch 160/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.2994\n",
            "Epoch 161/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2994\n",
            "Epoch 162/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2991\n",
            "Epoch 163/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2990\n",
            "Epoch 164/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.3015\n",
            "Epoch 165/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.2995\n",
            "Epoch 166/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2992\n",
            "Epoch 167/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2995\n",
            "Epoch 168/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.3001\n",
            "Epoch 169/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2988\n",
            "Epoch 170/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9854 - accuracy: 0.2993\n",
            "Epoch 171/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.3004\n",
            "Epoch 172/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2988\n",
            "Epoch 173/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2994\n",
            "Epoch 174/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9853 - accuracy: 0.2987\n",
            "Epoch 175/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2998\n",
            "Epoch 176/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.2996\n",
            "Epoch 177/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.3003\n",
            "Epoch 178/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.2993\n",
            "Epoch 179/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2992\n",
            "Epoch 180/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.3001\n",
            "Epoch 181/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.2997\n",
            "Epoch 182/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.3005\n",
            "Epoch 183/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2998\n",
            "Epoch 184/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.2989\n",
            "Epoch 185/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9852 - accuracy: 0.2996\n",
            "Epoch 186/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2996\n",
            "Epoch 187/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2999\n",
            "Epoch 188/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2995\n",
            "Epoch 189/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.3003\n",
            "Epoch 190/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2993\n",
            "Epoch 191/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2998\n",
            "Epoch 192/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2996\n",
            "Epoch 193/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.3009\n",
            "Epoch 194/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9850 - accuracy: 0.3002\n",
            "Epoch 195/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2983\n",
            "Epoch 196/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2986\n",
            "Epoch 197/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9851 - accuracy: 0.2994\n",
            "Epoch 198/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2992\n",
            "Epoch 199/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.3004\n",
            "Epoch 200/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.3002\n",
            "Epoch 201/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.3004\n",
            "Epoch 202/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2997\n",
            "Epoch 203/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2988\n",
            "Epoch 204/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2996\n",
            "Epoch 205/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.3010\n",
            "Epoch 206/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2998\n",
            "Epoch 207/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2999\n",
            "Epoch 208/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2995\n",
            "Epoch 209/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2998\n",
            "Epoch 210/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2989\n",
            "Epoch 211/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2996\n",
            "Epoch 212/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2997\n",
            "Epoch 213/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2983\n",
            "Epoch 214/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2999\n",
            "Epoch 215/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2995\n",
            "Epoch 216/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.2987\n",
            "Epoch 217/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.3002\n",
            "Epoch 218/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.2987\n",
            "Epoch 219/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.3006\n",
            "Epoch 220/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2998\n",
            "Epoch 221/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2994\n",
            "Epoch 222/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2994\n",
            "Epoch 223/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2993\n",
            "Epoch 224/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2999\n",
            "Epoch 225/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9848 - accuracy: 0.2989\n",
            "Epoch 226/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2998\n",
            "Epoch 227/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.2999\n",
            "Epoch 228/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2997\n",
            "Epoch 229/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9849 - accuracy: 0.3001\n",
            "Epoch 230/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.2988\n",
            "Epoch 231/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.3002\n",
            "Epoch 232/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.2987\n",
            "Epoch 233/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.3003\n",
            "Epoch 234/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2995\n",
            "Epoch 235/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2993\n",
            "Epoch 236/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.2989\n",
            "Epoch 237/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2989\n",
            "Epoch 238/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.3009\n",
            "Epoch 239/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.2997\n",
            "Epoch 240/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2997\n",
            "Epoch 241/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2995\n",
            "Epoch 242/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.3006\n",
            "Epoch 243/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.3003\n",
            "Epoch 244/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2994\n",
            "Epoch 245/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2997\n",
            "Epoch 246/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.3005\n",
            "Epoch 247/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.3000\n",
            "Epoch 248/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2999\n",
            "Epoch 249/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.3006\n",
            "Epoch 250/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.2988\n",
            "Epoch 251/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.3000\n",
            "Epoch 252/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2995\n",
            "Epoch 253/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.3000\n",
            "Epoch 254/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3006\n",
            "Epoch 255/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.3002\n",
            "Epoch 256/300\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.9845 - accuracy: 0.2999\n",
            "Epoch 257/300\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.9847 - accuracy: 0.2986\n",
            "Epoch 258/300\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.9845 - accuracy: 0.2996\n",
            "Epoch 259/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3011\n",
            "Epoch 260/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9843 - accuracy: 0.2990\n",
            "Epoch 261/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2995\n",
            "Epoch 262/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2997\n",
            "Epoch 263/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2998\n",
            "Epoch 264/300\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.9843 - accuracy: 0.2991\n",
            "Epoch 265/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9847 - accuracy: 0.2998\n",
            "Epoch 266/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.3000\n",
            "Epoch 267/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2993\n",
            "Epoch 268/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3004\n",
            "Epoch 269/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.3000\n",
            "Epoch 270/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2988\n",
            "Epoch 271/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9842 - accuracy: 0.3001\n",
            "Epoch 272/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2993\n",
            "Epoch 273/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2992\n",
            "Epoch 274/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2994\n",
            "Epoch 275/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.2998\n",
            "Epoch 276/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2993\n",
            "Epoch 277/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.3000\n",
            "Epoch 278/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2994\n",
            "Epoch 279/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9842 - accuracy: 0.3011\n",
            "Epoch 280/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2998\n",
            "Epoch 281/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2996\n",
            "Epoch 282/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2986\n",
            "Epoch 283/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3012\n",
            "Epoch 284/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9842 - accuracy: 0.3000\n",
            "Epoch 285/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3005\n",
            "Epoch 286/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2985\n",
            "Epoch 287/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9843 - accuracy: 0.2991\n",
            "Epoch 288/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9842 - accuracy: 0.2998\n",
            "Epoch 289/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9846 - accuracy: 0.3000\n",
            "Epoch 290/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2999\n",
            "Epoch 291/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3003\n",
            "Epoch 292/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3006\n",
            "Epoch 293/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3006\n",
            "Epoch 294/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.3004\n",
            "Epoch 295/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2997\n",
            "Epoch 296/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9842 - accuracy: 0.2994\n",
            "Epoch 297/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.2997\n",
            "Epoch 298/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9845 - accuracy: 0.3001\n",
            "Epoch 299/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9842 - accuracy: 0.3000\n",
            "Epoch 300/300\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2993\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb27058b990>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "X_test_avg = []\n",
        "for i in range(X_test.shape[0]):\n",
        "    X_test_avg.append(sentence_to_avg(X_test[i+30000].strip()))\n",
        "\n",
        "X_test_avg = np.array(X_test_avg)\n",
        "model.evaluate(X_test_avg, Y_test_oh)"
      ],
      "metadata": {
        "id": "WkIvwzRIHp1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ceb990-4075-479f-ac13-8ea3e1d70bc0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 2.1332 - accuracy: 0.2424\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.1332316398620605, 0.24240000545978546]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "X_me = np.array([\"not sad\", \"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy and funny\"])\n",
        "Y_me = np.array([[2], [0], [0], [2], [1], [4], [3]])\n",
        "X_me_avg = []\n",
        "\n",
        "for x in X_me:\n",
        "    X_me_avg.append(sentence_to_avg(x))\n",
        "\n",
        "X_me_avg = np.array(X_me_avg)\n",
        "pred = model.predict(X_me_avg)\n",
        "\n",
        "for i in range(X_me.shape[0]):\n",
        "    print(X_me[i], label_to_emoji(np.argmax(pred[i])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGsM6aAoHv2c",
        "outputId": "22ea9540-1c86-4bf3-fd95-53a275caf401"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 54ms/step\n",
            "not sad üòì\n",
            "i adore you üòÇ\n",
            "i love you üòÇ\n",
            "funny lol üòß\n",
            "lets play with a ball ü§¢\n",
            "food is ready ü§¢\n",
            "not feeling happy and funny üòÇ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emojifier-V2: Using RNNs: \n",
        "\n",
        "Let's build an LSTM model that takes as input word sequences. This model will be able to take word ordering into account. Emojifier-V2 will continue to use pre-trained word embeddings to represent words, but will feed them into an LSTM, whose job it is to predict the most appropriate emoji. \n",
        "\n",
        "Run the following cell to load the Keras packages.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://github.com/Alireza-Akhavan/rnn-notebooks/blob/master/images/emojifier-v2.png?raw=1\" style=\"width:700px;height:400px;\"> <br>\n",
        "<caption><center> **Figure 3**: Emojifier-V2. A 2-layer LSTM sequence classifier. </center></caption>"
      ],
      "metadata": {
        "id": "URuAB0T1llZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Alireza-Akhavan/rnn-notebooks/blob/master/images/embedding1.png?raw=1\" style=\"width:700px;height:250px;\">\n",
        "<caption><center> **Figure 4**: Embedding layer. This example shows the propagation of two examples through the embedding layer. Both have been zero-padded to a length of `max_len=5`. The final dimension of the representation is  `(2,max_len,50)` because the word embeddings we are using are 50 dimensional. </center></caption>"
      ],
      "metadata": {
        "id": "Cx-HnKjDTvNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "class EmojiNet_V2(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm_1 = GRU(128, return_sequences=True)\n",
        "        self.dropout_1 = Dropout(0.3)\n",
        "        self.lstm_2 = GRU(256)\n",
        "        self.dropout_2 = Dropout(0.5)\n",
        "        self.dense = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.lstm_1(x)\n",
        "        # x = self.dropout_1(x)\n",
        "        x = self.lstm_2(x)\n",
        "        x = self.dropout_2(x)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iac4qtchIXRk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model = EmojiNet_V2()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "HKURJ9USpxJR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the size of all sentences to max_len\n",
        "def convert_sentences_to_embeddings(X):\n",
        "    emb_dim = words_to_vec[\"cucumber\"].shape[0]  # define dimensionality of your GloVe word vectors (= 50)\n",
        "    emb_matrix = np.zeros((X.shape[0], max_len, emb_dim))\n",
        "    for i in range(X.shape[0]):\n",
        "        words = X[i].lower().split()\n",
        "        for j in range(len(words)):\n",
        "          try:\n",
        "            emb_matrix[i, j, :] = words_to_vec[words[j]]\n",
        "          except:\n",
        "            pass\n",
        "    return emb_matrix"
      ],
      "metadata": {
        "id": "hU4iX2_WH6jc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test convert_sentences_to_embeddings function\n",
        "X_me = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
        "print(X_me)\n",
        "print(convert_sentences_to_embeddings(X_me))"
      ],
      "metadata": {
        "id": "1CwvU_7FH_XB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db64d67-c47c-4ecd-c888-60bcec47b67b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['funny lol' 'lets play baseball' 'food is ready for you']\n",
            "[[[-0.014547 -0.20208  -0.75278  ... -0.13429   0.21133   1.5368  ]\n",
            "  [-0.54289   0.053743 -0.46978  ...  0.20745  -0.074958  0.080575]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  ...\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]]\n",
            "\n",
            " [[ 0.30423  -0.24405   1.0303   ... -0.43296  -0.096168  0.43463 ]\n",
            "  [-0.73571   0.19937  -0.89408  ... -0.075279 -0.44448   0.47437 ]\n",
            "  [-1.9327    1.0421   -0.78515  ...  0.55667  -0.70315   0.17157 ]\n",
            "  ...\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]]\n",
            "\n",
            " [[ 0.47222  -0.44545  -0.51833  ...  0.34932   0.33934   0.25499 ]\n",
            "  [ 0.6185    0.64254  -0.46552  ... -0.27557   0.30899   0.48497 ]\n",
            "  [ 0.36825  -0.20512   0.36656  ...  0.40331  -0.47358   0.54165 ]\n",
            "  ...\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run convert_sentences_to_embeddings function for training data \n",
        "X_train_embs =convert_sentences_to_embeddings(X_train)\n",
        "X_train_embs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci8WjOaRITIN",
        "outputId": "15b14cfd-4dd0-44c7-e845-8bebc0090f82"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 27, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_embs, Y_train_oh, epochs=100, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "kZp85tH8IdGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the size of all sentences to max_len\n",
        "def convert_sentences_to_embeddings(X):\n",
        "    emb_dim = words_to_vec[\"cucumber\"].shape[0]  # define dimensionality of your GloVe word vectors (= 50)\n",
        "    emb_matrix = np.zeros((X.shape[0], max_len, emb_dim))\n",
        "    for i in range(X.shape[0]):\n",
        "        words = X[i+30000].lower().split()\n",
        "        for j in range(len(words)):\n",
        "          try:\n",
        "            emb_matrix[i, j, :] = words_to_vec[words[j]]\n",
        "          except:\n",
        "            pass\n",
        "    return emb_matrix"
      ],
      "metadata": {
        "id": "TAJW0W0YQDiB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "X_test_embs = convert_sentences_to_embeddings(X_test)\n",
        "print(X_test_embs.shape)\n",
        "model.evaluate(X_test_embs, Y_test_oh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPCVVYx8ycym",
        "outputId": "e3e6d61d-bb2c-4519-8218-eb6b884de98f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 27, 50)\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 2.3654 - accuracy: 0.2942\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.365351915359497, 0.29420000314712524]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "X_me = np.array([\"not happy\", \"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy and funny\"])\n",
        "Y_me = np.array([[2], [0], [0], [2], [1], [4], [3]])\n",
        "X_me_embed = convert_sentences_to_embeddings(X_me) \n",
        "\n",
        "pred = model.predict(X_me_embed)\n",
        "\n",
        "for i in range(X_me.shape[0]):\n",
        "    print(X_me[i], label_to_emoji(np.argmax(pred[i])))"
      ],
      "metadata": {
        "id": "JYvYfbwfysLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52IiygWLzRLq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}